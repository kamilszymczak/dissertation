{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('dissertation': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dccb0c508fcc3265487c0045bf494404dc5148cb1c17dbd3e70d1418c59a7232"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "E:\\Program Files (x86)\\Anaconda3\\envs\\dissertation\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append('E:\\GitHubProjects\\dissertation\\Scripts')\n",
    "import helperfn as hf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = KeyedVectors.load_word2vec_format(r'E:\\GitHubProjects\\dissertation\\word2vec\\GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# If we don't plan to train the model any further, calling init_sims will make the model much more memory-efficient.\n",
    "embedding_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('good', 0.7190051078796387),\n",
       " ('terrible', 0.6828611493110657),\n",
       " ('horrible', 0.6702598333358765),\n",
       " ('Bad', 0.6698919534683228),\n",
       " ('lousy', 0.6647640466690063)]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "embedding_model.most_similar('bad', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Data\n",
    "data = hf.merge_datasets(r'E:\\GitHubProjects\\dissertation\\scraper\\approved_datasets')\n",
    "\n",
    "#Clean reivews (X)\n",
    "\n",
    "# #Remove stop words\n",
    "# X = X.apply(lambda x: hf.remove_punctuations(x))\n",
    "\n",
    "#Tokenizing the text using NLTK\n",
    "#Tokenize [TODO dont split apostrophe e.g. don't but students' ] \n",
    "# TODO ******. doesnt segment full stop \n",
    "# tokenizer = RegexpTokenizer(pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# https://stackoverflow.com/questions/42056872/how-to-remove-in-strings-with-regexptokenizer\n",
    "tokenizer = RegexpTokenizer(r'[\\w\\']+') #not seperating apostrophe\n",
    "\n",
    "\n",
    "data['tokenized_text'] = data['review'].apply(lambda x: tokenizer.tokenize(x)) \n",
    "\n",
    "#lower case\n",
    "data['tokenized_text'] = data['tokenized_text'].apply(lambda x: hf.lower_token(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f76263c65998>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_colwidth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "data['tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#counting number of unique words in  all reviews to get size of vocabulary\n",
    "corpus = set()\n",
    "for row in data['tokenized_text']:\n",
    "    corpus.update(row)\n",
    "vocab_size = len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = list(data['tokenized_text']), data['score']\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"tokenized_text\"], y, test_size=0.33, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Embeddings\n",
    "embedding_vecor_length = 300\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_vecor_length))\n",
    "\n",
    "for i, word in enumerate(corpus):\n",
    "    try:\n",
    "        embedding_vector = embedding_model[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "        # embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25), embedding_vecor_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_4\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_5 (InputLayer)            (None, 600)          0                                            \n__________________________________________________________________________________________________\nembedding_10 (Embedding)        (None, 600, 300)     737100      input_5[0][0]                    \n__________________________________________________________________________________________________\nconv1d_21 (Conv1D)              (None, 599, 200)     120200      embedding_10[0][0]               \n__________________________________________________________________________________________________\nconv1d_22 (Conv1D)              (None, 598, 200)     180200      embedding_10[0][0]               \n__________________________________________________________________________________________________\nconv1d_23 (Conv1D)              (None, 597, 200)     240200      embedding_10[0][0]               \n__________________________________________________________________________________________________\nconv1d_24 (Conv1D)              (None, 596, 200)     300200      embedding_10[0][0]               \n__________________________________________________________________________________________________\nconv1d_25 (Conv1D)              (None, 595, 200)     360200      embedding_10[0][0]               \n__________________________________________________________________________________________________\nglobal_max_pooling1d_21 (Global (None, 200)          0           conv1d_21[0][0]                  \n__________________________________________________________________________________________________\nglobal_max_pooling1d_22 (Global (None, 200)          0           conv1d_22[0][0]                  \n__________________________________________________________________________________________________\nglobal_max_pooling1d_23 (Global (None, 200)          0           conv1d_23[0][0]                  \n__________________________________________________________________________________________________\nglobal_max_pooling1d_24 (Global (None, 200)          0           conv1d_24[0][0]                  \n__________________________________________________________________________________________________\nglobal_max_pooling1d_25 (Global (None, 200)          0           conv1d_25[0][0]                  \n__________________________________________________________________________________________________\nconcatenate_5 (Concatenate)     (None, 1000)         0           global_max_pooling1d_21[0][0]    \n                                                                 global_max_pooling1d_22[0][0]    \n                                                                 global_max_pooling1d_23[0][0]    \n                                                                 global_max_pooling1d_24[0][0]    \n                                                                 global_max_pooling1d_25[0][0]    \n__________________________________________________________________________________________________\ndropout_9 (Dropout)             (None, 1000)         0           concatenate_5[0][0]              \n__________________________________________________________________________________________________\ndense_8 (Dense)                 (None, 128)          128128      dropout_9[0][0]                  \n__________________________________________________________________________________________________\ndropout_10 (Dropout)            (None, 128)          0           dense_8[0][0]                    \n__________________________________________________________________________________________________\ndense_9 (Dense)                 (None, 2)            258         dropout_10[0][0]                 \n==================================================================================================\nTotal params: 2,066,486\nTrainable params: 2,066,486\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding Layer, output dimension is 300 because word2vec 300d\n",
    "embedding_layer = Embedding(vocab_size, embedding_vecor_length, weights=[embedding_matrix], input_length=max_review_length, trainable=True)\n",
    "\n",
    "sequence_input = Input(shape=(max_review_length,))\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "convs = []\n",
    "filter_sizes = [2, 3, 4, 5, 6]\n",
    "for filter_size in filter_sizes:\n",
    "    l_conv = Conv1D(filters=200, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "    l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "    convs.append(l_pool)\n",
    "\n",
    "l_merge = concatenate(convs, axis=1)\n",
    "x = Dropout(0.1)(l_merge)  \n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(2, activation='sigmoid')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_3 to have shape (600,) but got array with shape (1,)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-ab9b8484b044>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Program Files (x86)\\Anaconda3\\envs\\dissertation\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files (x86)\\Anaconda3\\envs\\dissertation\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files (x86)\\Anaconda3\\envs\\dissertation\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_3 to have shape (600,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Embedding Layer, output dimension is 300 because word2vec 300d\n",
    "embedding_layer = Embedding(vocab_size, embedding_vecor_length, weights=[embedding_matrix], input_length=max_review_length), trainable=True)\n",
    "\n",
    "sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "\n",
    "model.add(Convolution1D(64, 3, border_mode='same'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Convolution1D(16, 3, border_mode='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(180,activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "#Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#Display summary of the model\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          epochs=nb_epochs,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[EarlyStopping(min_delta=0.00025, patience=2)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_cnn = Sequential()\n",
    "# e = Embedding(100000, 100, input_length=1000)\n",
    "# model_cnn.add(e)\n",
    "# model_cnn.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "# model_cnn.add(GlobalMaxPooling1D())\n",
    "# model_cnn.add(Dense(256, activation='relu'))\n",
    "# model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "# model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model_cnn.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)\n",
    "# score,acc = model_cnn.evaluate(x_val_seq, y_validation, verbose = 2, batch_size = 32)\n",
    "# print(\"score: %.2f\" % (score))\n",
    "# print(\"acc: %.2f\" % (acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "#Get Data\n",
    "data = hf.merge_datasets(r'E:\\GitHubProjects\\dissertation\\scraper\\approved_datasets')\n",
    "# data = hf.balance_dataset(data)\n",
    "\n",
    "#Clean reivews (X)\n",
    "\n",
    "# #Remove stop words\n",
    "# X = X.apply(lambda x: hf.remove_punctuations(x))\n",
    "\n",
    "#Tokenizing the text using NLTK\n",
    "#Tokenize [TODO dont split apostrophe e.g. don't but students' ] \n",
    "# TODO ******. doesnt segment full stop \n",
    "# https://stackoverflow.com/questions/42056872/how-to-remove-in-strings-with-regexptokenizer\n",
    "tokenizer = RegexpTokenizer(r'[\\w\\']+') #not seperating apostrophe\n",
    "\n",
    "\n",
    "data['tokenized_text'] = data['review'].apply(lambda x: tokenizer.tokenize(x)) \n",
    "\n",
    "#remove stop words\n",
    "data['tokenized_text'] = data['tokenized_text'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "#lower case\n",
    "data['tokenized_text'] = data['tokenized_text'].apply(lambda x: hf.lower_token(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = list(data['review']), list(data['score'])\n",
    "X, y = data['tokenized_text'], list(data['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#vocab size?\n",
    "NUM_WORDS = 10000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the maximum number of words the longests sentence contains\n",
    "# length = []\n",
    "# # TODO X_train might need to be a list not dataframe [\"aa\", \"bb\"]\n",
    "# for x in X_train:\n",
    "#     length.append(len(x.split()))\n",
    "# max(length)\n",
    "\n",
    "#my more memory efficient version\n",
    "# longest = 0\n",
    "# for x in X_train:\n",
    "#     if len(x.split()) > longest:\n",
    "#         longest = len(x.split())\n",
    "\n",
    "#VERSION FOR TOKENIZED ROWS\n",
    "longest = 0\n",
    "for x in X:\n",
    "    if len(x) > longest:\n",
    "        longest = len(x)\n",
    "\n",
    "\n",
    "#padding sequences to have the same length\n",
    "x_train_seq = pad_sequences(sequences, maxlen=longest+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_val = tokenizer.texts_to_sequences(X_test)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=longest+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train_seq, y, test_size=0.33, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Embeddings\n",
    "#length of vectors in embedding word2vec\n",
    "embedding_vecor_length = 300\n",
    "embedding_matrix = np.zeros((NUM_WORDS, embedding_vecor_length))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = embedding_model[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "        # embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25), embedding_vecor_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import MaxPooling1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 1095 samples, validate on 540 samples\n",
      "Epoch 1/7\n",
      " - 0s - loss: 0.5654 - accuracy: 0.7114 - val_loss: 0.4151 - val_accuracy: 0.8519\n",
      "Epoch 2/7\n",
      " - 0s - loss: 0.2842 - accuracy: 0.9023 - val_loss: 0.2990 - val_accuracy: 0.8722\n",
      "Epoch 3/7\n",
      " - 0s - loss: 0.1650 - accuracy: 0.9388 - val_loss: 0.2620 - val_accuracy: 0.8981\n",
      "Epoch 4/7\n",
      " - 0s - loss: 0.0922 - accuracy: 0.9781 - val_loss: 0.2610 - val_accuracy: 0.9037\n",
      "Epoch 5/7\n",
      " - 0s - loss: 0.0464 - accuracy: 0.9936 - val_loss: 0.2661 - val_accuracy: 0.9056\n",
      "Epoch 6/7\n",
      " - 0s - loss: 0.0254 - accuracy: 0.9973 - val_loss: 0.3042 - val_accuracy: 0.9000\n",
      "Epoch 7/7\n",
      " - 0s - loss: 0.0161 - accuracy: 0.9973 - val_loss: 0.2803 - val_accuracy: 0.9019\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2d03eecb5c8>"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "model_cnn = Sequential()\n",
    "e = Embedding(NUM_WORDS, 300, weights=[embedding_matrix], input_length=longest+5, trainable=False)\n",
    "model_cnn.add(e)\n",
    "model_cnn.add(Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(256, activation='relu'))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=7, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test loss: 0.28029762616863957 / Test accuracy: 0.9018518328666687\n"
     ]
    }
   ],
   "source": [
    "# tk_test = Tokenizer()\n",
    "# tk_test.fit_on_texts(X_test)\n",
    "# index_list = tk_test.texts_to_sequences(X_test)\n",
    "# x_train_padded = pad_sequences(index_list, maxlen=longest+5)\n",
    "\n",
    "score = model_cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[147,  33],\n",
       "       [ 20, 340]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = (model_cnn.predict(X_test).ravel()>0.5)+0 # predict and get class (0 if pred < 0.5 else 1)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}