{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('dissertation': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dccb0c508fcc3265487c0045bf494404dc5148cb1c17dbd3e70d1418c59a7232"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "E:\\Program Files (x86)\\Anaconda3\\envs\\dissertation\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "\n",
    "sys.path.append('E:\\GitHubProjects\\dissertation\\Scripts')\n",
    "import helperfn as hf\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Data\n",
    "data = hf.merge_datasets(r'E:\\GitHubProjects\\dissertation\\scraper\\approved_datasets')\n",
    "\n",
    "#Clean reivews (X)\n",
    "\n",
    "# #Remove stop words\n",
    "# X = X.apply(lambda x: hf.remove_punctuations(x))\n",
    "\n",
    "#Tokenize [TODO dont split apostrophe e.g. don't but students' ] \n",
    "# TODO ******. doesnt segment full stop \n",
    "tokenizer = RegexpTokenizer(pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "\n",
    "data['tokenized_text'] = data['review'].apply(lambda x: tokenizer.tokenize(x)) \n",
    "\n",
    "#lower case\n",
    "data['tokenized_text'] = data['tokenized_text'].apply(lambda x: hf.lower_token(x))\n",
    "\n",
    "\n",
    "#Word Embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X, y = hf.get_data(data)\n",
    "# Split the data\n",
    "# x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, shuffle= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                review  score  \\\n",
       "0    A shambles, completely unorganised and clearly...      0   \n",
       "1    This school is absolute b******. I'm in the MS...      0   \n",
       "2    As an international student, lecturers are gre...      1   \n",
       "3    I have had a terrible experience at this unive...      0   \n",
       "4    I do love my university. The staff are extreme...      1   \n",
       "..                                                 ...    ...   \n",
       "363  Fantastic environment and gorgeoud scenery. Th...      1   \n",
       "364  The campus contains everything you could ever ...      1   \n",
       "365  The campus is beautiful and it offers over 100...      1   \n",
       "366  I'm study in the highland campus in Inverness....      0   \n",
       "367  Wifi drops off easily and applying for accommo...      0   \n",
       "\n",
       "                                        tokenized_text  \n",
       "0    [a, shambles, ,, completely, unorganised, and,...  \n",
       "1    [this, school, is, absolute, b, ******., i, 'm...  \n",
       "2    [as, an, international, student, ,, lecturers,...  \n",
       "3    [i, have, had, a, terrible, experience, at, th...  \n",
       "4    [i, do, love, my, university, ., the, staff, a...  \n",
       "..                                                 ...  \n",
       "363  [fantastic, environment, and, gorgeoud, scener...  \n",
       "364  [the, campus, contains, everything, you, could...  \n",
       "365  [the, campus, is, beautiful, and, it, offers, ...  \n",
       "366  [i, 'm, study, in, the, highland, campus, in, ...  \n",
       "367  [wifi, drops, off, easily, and, applying, for,...  \n",
       "\n",
       "[368 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>score</th>\n      <th>tokenized_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A shambles, completely unorganised and clearly...</td>\n      <td>0</td>\n      <td>[a, shambles, ,, completely, unorganised, and,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This school is absolute b******. I'm in the MS...</td>\n      <td>0</td>\n      <td>[this, school, is, absolute, b, ******., i, 'm...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>As an international student, lecturers are gre...</td>\n      <td>1</td>\n      <td>[as, an, international, student, ,, lecturers,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I have had a terrible experience at this unive...</td>\n      <td>0</td>\n      <td>[i, have, had, a, terrible, experience, at, th...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I do love my university. The staff are extreme...</td>\n      <td>1</td>\n      <td>[i, do, love, my, university, ., the, staff, a...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>363</th>\n      <td>Fantastic environment and gorgeoud scenery. Th...</td>\n      <td>1</td>\n      <td>[fantastic, environment, and, gorgeoud, scener...</td>\n    </tr>\n    <tr>\n      <th>364</th>\n      <td>The campus contains everything you could ever ...</td>\n      <td>1</td>\n      <td>[the, campus, contains, everything, you, could...</td>\n    </tr>\n    <tr>\n      <th>365</th>\n      <td>The campus is beautiful and it offers over 100...</td>\n      <td>1</td>\n      <td>[the, campus, is, beautiful, and, it, offers, ...</td>\n    </tr>\n    <tr>\n      <th>366</th>\n      <td>I'm study in the highland campus in Inverness....</td>\n      <td>0</td>\n      <td>[i, 'm, study, in, the, highland, campus, in, ...</td>\n    </tr>\n    <tr>\n      <th>367</th>\n      <td>Wifi drops off easily and applying for accommo...</td>\n      <td>0</td>\n      <td>[wifi, drops, off, easily, and, applying, for,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>368 rows Ã— 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "#input_length - should not be larger than vocabulary size\n",
    "max_length = 100\n",
    "\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "#Display summary of the model\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          shuffle=True,\n",
    "          epochs=nb_epochs,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          callbacks=[EarlyStopping(min_delta=0.00025, patience=2)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_cnn = Sequential()\n",
    "e = Embedding(100000, 100, input_length=1000)\n",
    "model_cnn.add(e)\n",
    "model_cnn.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(256, activation='relu'))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)\n",
    "score,acc = model_cnn.evaluate(x_val_seq, y_validation, verbose = 2, batch_size = 32)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))\n"
   ]
  }
 ]
}